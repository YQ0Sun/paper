<center>
    <h1>
        Going deeper with convolutions
    </h1>
</center>


## 摘要    

提出一种深度卷积神经网络体系结构。结构的设计允许在保持计算预算不变的情况下增加网络的深度和宽度，从而提高了网络内部计算资源的利用率。为了优化质量，架构决策基于赫比安原则和多尺度处理的直觉。

## 1. 引言

深度学习的进步不仅仅是更强大的硬件、更大的数据集和更大的模型的结果，而且主要是新的想法、算法和改进的网络架构的结果。

本文的算法不仅注重精度，还注重计算速度，以保证在现实世界的使用。

deep有两层含义：第一个层引入新的组织层次以“Inception module”的形式，第二层就是增加网络深度。

代码

复现：[alexnet-pytorch/model.py at master · dansuh17/alexnet-pytorch · GitHub](https://github.com/dansuh17/alexnet-pytorch/blob/master/model.py)

## 2. 相关工作

从LeNet-5[10]开始，卷积神经网络(CNN)通常使用堆叠一系列卷积层(可选地跟随归一化和最大池化)后面是一个或多个全连接层。对于更大的数据集，如Imagenet，最近的趋势是增加层数和层大小，同时使用Dropout[7]来解决过度拟合的问题。

Inspired思想是使用不同尺度的卷积核进行处理多种尺度，类似于Inception model。GoogLeNet的22层deep层（Inception module）都是用来学习的。

Network-in-Network可以被看作是附加的1×1卷积层，然后通常是经过校正的线性激活[9]。1×1卷积具有双重用途：主要用作降维模块，以消除计算瓶颈，否则将限制网络的大小，第二个作用可以增加网络的深度，还可以增加网络的宽度，而不会对性能造成显著影响。

目前主要的目标检测方法是由Girshick等人提出的区域卷积神经网络(R-CNN)。R-CNN将整个检测问题分解为两个子问题：首先以一种与类别无关的方式利用颜色和超像素一致性等低层线索（具有边界框分割的准确性）来识别潜在的对象建议，然后使用CNN分类器来识别这些位置的对象类别。GoogLeNet采用了类似的流程，但是都探索了增强的功能，用于更高对象边界框recell的多框预测，以及用于更好地对边界框建议进行分类的集成方法。

## 3. 动机和高层考虑

提高深度神经网络性能的最直接方法是增加它们的规模。这包括增加网络的深度：网络层数，还有宽度：每一层的单元数量。但是有两个缺陷：更大的规模通常意味着更多的参数，这使得扩大的网络更容易过拟合，特别是在训练集中标记的样本数量有限的情况下。统一增加网络规模的另一个缺点是显著增加了计算资源的使用。

解决这两个问题的根本方法是最终从完全连接的体系结构过渡到稀疏连接的体系结构，甚至在卷积内部也是如此。理论上稀疏网络更高效，但在真实硬件上反而更慢；因此现代 CNN卷积网络本身为“空间稀疏”，但在实现上仍然使用致密矩阵计算。卷积被称为空间稀疏的原因是它没有连接导全图，只和局部的像素点连接，小核相当于稀疏感受野。

提议：实现理论层面所建议的额外稀疏性，甚至在卷积层面级别，但通过利用稠密矩阵上的计算来利用我们现有的硬件。

## 4 架构细节

如何把“理论上最优的稀疏结构”用“现实中可实现的致密卷积”来近似实现。

Arora 的理论思想聚为一组相关性较高的单元。统计前一层神经元的相关性，把高度相关的神经元聚成一簇 cluster，每一簇 → 下一层的一个卷积单元，相关性高 → 局部集中。

Inception 的想法用多个不同大小的卷积核（1、局部：小卷积核。2、跨空间：大核卷积。3、越大的区域：越少的簇）并联，再把输出拼起来，形成下一级的输入。来近似这种稀疏、分组、局部相关的真实结构。

将“Inception modules”堆叠起来，输出相关性统计数据必然会发生变化：随着更高抽象的特征被更高的层捕获，它们的空间集中度预计会降低，这表明随着我们移动到更高的层，3×3和5×5卷积的比率应该增加。

上述模块的一个大问题是，在具有大量卷积层时，导致各阶段的输出数量增加。这就引出了提议的体系结构的第二个想法：明智地在计算需求否则会增加太多的地方应用降维和投影。我们希望在大多数地方保持我们的表示稀疏，并且只在信号必须聚集在一起时才对它们进行压缩。也就是说，在昂贵的3×3和5×5卷积之前，使用1×1卷积来计算约化。除了用作还原，它们还包括使用整流线性激活，这使它们具有双重用途。最终结果如图2(B)所示。

![1767425982773](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1767425982773.png)

一般来说，Inception network是由上述类型的模块相互堆叠组成的网络。但由于技术原因，只在较高层开始使用Inception model，同时保持较低层的传统卷积方式似乎是有益的。这种体系结构的主要好处之一是，它允许在每个阶段显著增加单元数量，而不会在计算复杂性方面出现不受控制的爆炸。降维的普遍使用首先降低它们的维度，然后用大片尺寸卷积在它们上面。它符合视觉信息应该在不同的尺度上进行处理，然后聚集在一起，以便下一阶段可以同时从不同的尺度提取特征。

## 5. GoogLeNet



![1767426653248](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1767426653248.png)

所有的卷积，都使用了线性激活函数。在我们的网络中，接受视野的大小是224×224，使用预处理：RGB-均值。#3×3约简和#5×5约简代表在3×3和5×5卷积之前使用1个1×1卷积层。

将全连接层改为平均池化层会提高精度。

较浅的网络的梯度回传能力很强，网络中间层产生的特征具有很强的区分性，可以添加连接到这些中间层的辅助分类器，期望在分类器的较低阶段鼓励区分，以增强回传的梯度信号，并提供额外的正则化。这些分类器采用较小的卷积网络的形式，放在初始(4a)和(4d)模块的输出中。在训练期间，损失被添加到具有折扣权重的网络的总损失中(辅助分类器的损失被加权0.3)。在推理时，这些辅助网络被丢弃。

包括辅助分类器在内的边上额外网络的确切结构如下：

- 具有5×5卷积核stride为3的平均池化，得到4×4×512的(4a)阶段和4×4×528的(4d)阶段。
- 128个1×1卷积，用于降维和校正线性激活。
- 一个全连接层具有1024个单元和整流线性激活。
- 70%丢弃输出率的退出层。
- Softmax Loss作为分类器的线性层(预测与主分类器相同的1000个类别，但在推理时删除)。

结果网络的示意图如图3所示。

![1767427700459](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1767427700459.png)

![1767427729307](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1767427729307.png)

![1767427753678](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1767427753678.png)

## 6. 值得学习的点

1、利用不同的卷积的拼接实现了多尺度，减少计算量还增加了网络的深度和宽度，利用硬件的稠密矩阵近似实现理论上的稀疏矩阵（空间稀疏）。