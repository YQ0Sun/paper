<center>
    <h1>
        Visualizing and Understanding Convolutional Networks
    </h1>
</center>

## 摘要    

介绍了一种新的可视化技术，它可以深入了解中间特征层的功能和分类器的操作。这些可视化用于诊断角色，使我们能够找到性能优于Krizevsky等人的模型体系结构。在ImageNet分类基准上。我们还进行了烧蚀研究，以发现不同模型层对性能的贡献。我们展示了我们的ZFNet模型很好地推广到其他数据集：当Softmax分类器被重新训练时，它令人信服地击败了在Caltech-101和Caltech-256数据集上的当前最先进的结果。

## 1. 引言

在本文中，我们介绍了一种**可视化技术**，它揭示了激励模型中任何层的单个特征地图的输入刺激。它还允许我们**观察训练过程中特征的演变，并诊断模型的潜在问题**。我们提出的可视化技术使用一个**多层去卷积网络**，如Zeiler等人所提出的那样，将特征激活投影回输入像素空间。我们还通过遮挡输入图像的部分来执行对分类器输出的敏感度分析，揭示场景的哪些部分对于分类是重要的。使用这些工具，我们从Krizevsky等人的体系结构开始，探索不同的体系结构，发现其中一个比他们在ImageNet上的结果更好。然后，我们探索了该模型对其他数据集的泛化能力，只需在顶部重新训练Softmax分类器。因此，这是一种监督预培训的形式，与Hinton等人和其他人Bengio等人；文森特等人推广的无监督预培训方法形成了对比。ConvNet特征的泛化能力也在并发工作中进行了探索Donahue等人。

代码

复现：[alexnet-pytorch/model.py at master · dansuh17/alexnet-pytorch · GitHub](https://github.com/dansuh17/alexnet-pytorch/blob/master/model.py)

## 2. 相关工作

可视化功能以获得对网络的直观是常见的做法，但主要限于第一层，在那里可以投影到像素空间。在更高的层面上，情况并非如此，解释活动的方法有限。通过在图像空间中**执行梯度下降来最大化单元的激活来寻找每个单元的最优刺激**。这需要仔细的初始化，并且不会给出关于单位不变性的任何信息。在后者不足的激励下，Le等人扩展了Berkes&Wiskott的想法，展示了给定单位的黑森可以如何围绕最优响应进行数值计算，从而提供了对不变性的一些见解。问题是，对于更高的层，不变性是极其复杂的，所以很难用简单的二次近似来捕捉。相反，我们的方法提供了一个**非参数的不变性视图**，显示了来自训练集中的哪些模式激活了特征地图。Donahue等人展示了识别数据集中负责模型中较高层强烈激活的补丁的可视化。我们的可视化不同之处在于，它们**不仅是输入图像的裁剪，而且是自上而下的投影**，揭示了每个补丁中刺激特定特征地图的结构。传统的物体识别方法依赖于机器学习技术，但受限于数据集的大小和多样性。为了提高识别性能，研究者们转向了更大规模的数据集，如 ImageNet，它包含了超过 1500 万张标记图像。

## 3. 方法

我们在整个论文中使用标准的全监督ConvNet模型，如LeCun等人，和Krizevsky等人，2012年所定义的那样。这些模型通过一系列层将彩色2D输入图像x~i~映射到C个不同类别上的概率向量$\vec{y_i}$。每一层包括(i)前一层输出与一组学习过滤器的卷积；(ii)通过校正的线性函数(relu(X)=max(x，0))传递响应；(Iii)[可选的]局部邻域上的最大汇集和(iv)[可选的]局部对比度运算，其跨特征映射归一化响应。关于这些行动的更多细节，见Krizevsky等人和Jarrett等人。网络的顶层是传统的全连接网络，最后一层是Softmax分类器。图3显示了我们的许多实验中使用的模型。我们使用一大组N个标记的图像{x，y}来训练这些模型，其中，标签y~i~是指示真实类别的离散变量。采用一种适用于图像分类的交叉熵损失函数来比较y~i~和$\vec{y_i}$。通过反向**传播损耗**相对于整个网络中的参数的导数，并通过随机梯度下降来更新参数，来训练网络的参数(卷积层中的过滤器、全连通层中的权重矩阵和偏差)。

![1766924421418](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766924421418.png)

图3.我们的8层ConvNet模型的体系结构图像的224 x 224裁剪(具有3个颜色平面)被呈现为输入。这与96个不同的第一层滤光片(红色)卷积，每个滤光片的大小为7×7，使用x和y中的步长为2。所得到的特征图随后是：(I)通过校正的线性函数(未示出)，(Ii)最大池化和(Iii)跨特征图的对比度归一化，以得到96个不同的55×55元素特征图。在第2、3、4、5层中重复类似的操作。最后两层是完全连接的，以矢量形式(6·6·256=9216维)从顶卷积层获取特征作为输入。最后一层是C-Way Softmax函数，C是类数。所有的过滤器和特征地图都是正方形的。

### 3.1使用Deconvnet实现可视化

要理解转换网的操作，需要解释中间层中的功能活动。我们提出了一种新的方法来将这些活动映射回输入像素空间，显示什么输入模式最初导致了特征映射中的给定激活。我们使用去卷积网络(Deconvnet)来执行这种映射(Zeiler等人，2011年)。Deconvnet可以被认为是使用相同组件(过滤、池化)的ConvNet模型，但反过来，与将像素映射到特征相反。Deconvnet被提出作为一种进行无监督学习的方式。在这里，它们不用于任何学习能力，就像一个已经接受过训练的ConvNet的探测器。为了检查ConvNet，在它的每一层上都附加了一个Deconvnet，如图1(上图)所示，提供了一条返回图像像素的连续路径。首先，将输入图像呈现给ConvNet，并计算整个层的特征。为了检查给定的ConvNet激活，我们将层中的所有其他激活设置为零，并将功能地图作为输入传递到附加的Deconvnet层。然后，我们依次(I)unpool解聚、(Ii)rectify整流和(Iii)filter过滤，以重建导致所选激活的下层中的活性。然后重复这一过程，直到达到输入像素空间。

![1766922716143](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766922716143.png)

图1.连接到ConvNet层的Deconvnet层(左)(右)。Deonvnet将从下面的层重建Convnet要素的近似版本。底部：使用开关记录在ConvNet中汇集期间每个汇集区域(有色区域)中的局部最大值的位置，这是在解合并网络中的解汇集操作的图示。

**unpool解聚**：转置卷积，在ConvNet中，最大合并操作是不可逆的，但我们可以通过在一组开关变量中记录最大值在每个合并区域中的位置来获得近似逆。在解网络中，解合操作使用这些开关将来自上面层的重建放置到适当的位置，保留刺激的结构。有关该过程的说明，请参见图1(底部)。

**rectification整流：**ConvNet使用RELU非线性，从而纠正特征映射，从而确保特征映射始终为正。为了在每一层获得有效的特征重建(也应该是正的)，我们通过RELU非线性传递重建的信号。

**filter过滤：**ConvNet使用学习的过滤器来卷积来自上一层的要素地图。为了反转这一点，反滤镜使用了相同滤镜的转置版本，但应用于校正后的地图，而不是下面层的输出。实际上，这意味着垂直和水平翻转每个滤镜。

从更高层向下投影使用向上的ConvNet中的最大池生成的开关设置。由于这些开关设置是给定输入图像所特有的，因此从单个激活获得的重建类似于原始输入图像的一小部分，具有根据其对特征激活的贡献来加权的结构。由于模型是区分训练的，因此它们隐含地显示了输入图像的哪些部分是区分的。请注意，这些预测不是来自模型的样本，因为不涉及生成过程。

## 4. 培训细节

我们现在描述将在第5节中可视化的大型ConvNet模型。图3所示的体系结构类似于Krizevsky等人用于ImageNet分类的体系结构。一个不同之处是，Krizevsky的第3、4、5层(由于模型被拆分到2个GPU上)中使用的稀疏连接被我们的模型中的密集连接所取代。如5.1节所述，在检查图6中的可视化之后，与层1和层2相关的其他重要区别被作出。

![1766924829354](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766924829354.png)

图6：没有要素比例裁剪的第一层要素。请注意，有一项功能占主导地位。(B)：第1层特写(Krizevsky等人，2012年)。(C)：我们的第一层功能。步幅(2比4)和滤镜大小(7x7比11x11)越小，特征越明显，“死”特征越少。(D)：第二层特征的可视化。(E)：我们第二层功能的可视化。它们更干净，没有(D)中可见的锯齿瑕疵。

该模型在ImageNet 2012培训集上进行了培训(130万张图像，分布在1000个不同的班级)。通过将最小维度调整为256，裁剪中心256x256区域，减去每个像素的平均值(跨越所有图像)，然后使用大小为224x224的10个不同的子裁剪(角+带(外)水平翻转的中心)，对每幅RGB图像进行预处理。使用小批量为128时的随机梯度下降来更新参数，学习速率为10−2，结合动量项为0.9.当验证误差趋于平台期时，我们对整个训练过程中的学习率进行人工退火法。在完全连接层(6和7)中以0.5的速率使用丢弃(Hinton等人，2012)。将所有权重初始化为10−2，并将偏移设置为0。

在训练期间对第一层过滤器的可视化显示，其中一些过滤器占主导地位，如图6(A)所示。为了解决这一问题，我们将有效值超过固定半径10−1的卷积层中的每个滤波器重新正规化到这个固定半径。这一点至关重要，尤其是在模型的第一层，其中输入图像大致在[-128,128]范围内。与在(Krizevsky等人，2012年)中一样，我们生产了多个不同的作物和每个训练样本的翻转，以扩大训练集的规模。我们在70个纪元后停止了培训，在一个GTX580 GPU上大约花了12天的时间，使用基于Krizevsky等人的实现(Krizevsky等人，2012年)。

这是作者认为网络中最重要的一环，使用的Relus激活函数。文中通过实验证明在同样的模型训练中，非饱和线性Relus的效率要高于饱和线性tanh函数（效率指达到相同准确率需要的epoch数量）。文中还强调更快的训练对于大的数据集和模型都是很大的积极作用的。

## 5. 转换网络可视化

使用第3节中描述的模型，我们现在使用Deconvnet来可视化ImageNet验证集上的功能激活。

**功能可视化：**图2显示了培训完成后我们模型中的功能可视化。然而，我们没有显示给定功能地图的最强激活，而是显示了前9个激活。将它们分别向下投影到像素空间，揭示了激发给定特征图的不同结构，从而显示了其对输入变形的不变性。除了这些可视化效果，我们还显示了相应的**图像补丁**。这些比可视化有更大的变化，因为后者只关注**每个斑块内的判别结构**。例如，在第5层、第1行、第2列中，补丁似乎**没有什么共同之处**，但可视化显示，该特定特征地图关注的是背景中的草地，而不是前景对象。



**来自每一层的投影显示了网络中要素的分层性质。**第2层响应角点和其他边缘/颜色连接。层3具有更复杂的不变性，捕捉相似的纹理(例如网格图案(行1，列1)；文本(R2，C4))。第4层显示出显著的变化，但更特定于类别：狗脸(R1，C1)；鸟腿(R4，C2)。第5层显示具有显著姿势变化的整个对象，例如键盘(R1、C11)和狗(R4)。

对于每个功能地图，我们还会显示相应的图像补丁。注：(I)每个特征映射内的强分组，(Ii)更高层的更大不变性，以及(Iii)图像的可区分部分的夸大，例如狗的眼睛和鼻子(第4层，第1行，COLS 1)。

**训练期间的特征演变：**图4可视化了在投影回像素空间的给定特征映射内的最强激活(跨越所有训练实例)在训练期间的进展。**外观上的突然跳跃是由于产生最强烈激活的图像的变化造成的**。可以看到，模型的较低层在几个时期内收敛。然而，上层只有在相当多的时期(40-50)之后才会发展，这表明需要让模型训练直到完全收敛。

![1766924536352](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766924536352.png)

图4.通过训练随机选择的模型特征子集的演变。每个层的要素显示在不同的块中。在每个块中，我们显示了在历元[1，2，5，10，20，30，40，64]处随机选择的特征子集。可视化显示了给定特征地图的最强激活(在所有训练示例中)，并使用我们的Deconvnet方法向下投影到像素空间。色彩对比度被人工增强，图形以电子形式观看效果最佳。

**特征不变性：**图5示出了5个样本图像被不同程度地平移、旋转和缩放，同时查看来自模型的顶层和底层的特征向量相对于未变换特征的变化。**小的变换在模型的第一层有很大的影响，但在顶层的影响较小**，对于平移和缩放是准线性的。**网络输出对于转换和缩放是稳定的。**一般而言，除了具有旋转对称性的对象(例如，娱乐中心)外，**输出不随旋转不变。**

![1766924631859](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766924631859.png)

图5.模型中垂直平移、比例和旋转不变性的分析(分别为第a-c行)。COL 1：5正在进行变换的图像示例。Col 2&3：分别来自第1层和第7层的原始图像和变换图像的特征向量之间的欧几里得距离。第4列：变换图像时，每个图像的真实标签的概率。

### 5.1 架构选择

虽然对经过训练的模型进行可视化可以深入了解其操作，但它也可以帮助选择好的体系结构。通过可视化Krizevsky等人的第一层和第二层。的架构(图6(B)和(D))，各种问题显而易见。第一层滤波器是极高和极低频率信息的混合，几乎不覆盖中频。此外，第二层可视化显示了由第一层旋转中使用的大步幅4引起的锯齿伪影。为了解决这些问题，我们(I)将第一层滤波器的尺寸从11x11减小到7x7，(Ii)使卷积2而不是4迈出了一大步。这种新的体系结构在第一层和第二层特征中保留了更多的信息，如图6(C)和(E)所示。更重要的是，它还提高了分类性能，如5.1节所示。

### 5.2 遮挡敏感性

对于图像分类方法，一个自然的问题是，模型是真正识别图像中对象的位置，还是仅仅使用周围的上下文。图7试图通过用灰色正方形系统地遮挡输入图像的不同部分并监视分类器的输出来回答这个问题。这些示例清楚地表明，该模型正在定位场景中的对象，因为当对象被遮挡时，正确类的概率会显著下降。图7还示出了来自顶卷积层的最强特征地图的可视化，以及作为**遮挡器位置的函数的该地图中的活动(在空间位置上求和)**。当遮挡物覆盖可视化中出现的图像区域时，我们会看到特征地图中的活跃度显著下降。这表明可视化确实对应于刺激该特征图的图像结构，从而验证了图4和图2中所示的其他可视化。

### 5.3 对应分析

深度模型与许多现有的识别方法的不同之处在于，没有明确的机制来在不同图像中的特定对象部分之间建立对应(例如，人脸具有特定的眼睛和鼻子的空间配置)。然而，一个耐人寻味的可能性是，深度模型可能正在隐式计算它们。为了探索这一点，我们随机抽取了5张正面姿势的狗狗图像，并系统地遮盖了每张图像中相同的面部部分(例如，所有的左眼，见图8)。然后，对于每个图像i，我们计算：

![1766925473335](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766925473335.png)

$x^l_i$和$\vec{x}^l_i​$分别是原始图像和遮挡图像在l层的特征向量。然后，我们测量所有相关图像对(i，j)之间的该差异向量的一致性：

![1766925658018](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766925658018.png)

其中H是汉明距离。较低的值表示遮罩操作产生的更改的一致性更高，因此不同图像中相同对象部分之间的对应关系更紧密(即，阻挡左眼以一致的方式更改特征表示)。在表1中，我们使用L=5和L=7的层特征，将面部三个部分(左眼、右眼和鼻子)的∆分数与对象的随机部分进行比较。对于层5特征，这些部分相对于随机对象区域的较低分数表明该模型确实建立了某种程度的对应关系。

![1766925758185](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766925758185.png)

图8.用于通信实验的图像。第一栏：原图。第2、3、4列：右眼、左眼、鼻子分别闭塞。其他列显示了随机遮挡的示例。

表1.5种不同的狗形象中不同物体部位的对应关系。眼睛和鼻子的较低分数(与随机对象部分相比)表明该模型隐含地建立了模型中第5层部分的某种形式的对应关系。在第七层，分数更接近，可能是因为上层试图区分不同品种的狗。

![1766925790838](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766925790838.png)

## 6.值得学习的点

1、提出一种可视化卷积网络中间层特征图的方法。