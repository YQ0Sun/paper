<center>
</center>

# ImageNet Classification with Deep Convolutional Neural Networks

## 摘要    

我们训练了一个大型的深度卷积神经网络，将ImageNet LSVRC-2010竞赛中的120万张高分辨率图像分类为1000个不同的类别。在测试数据上，我们获得了前1名和前5名的错误率分别为37.5%和17.0%，这比以前的技术水平要好得多。该神经网络有6000万个参数和65万个神经元，由五个<b>卷积层</b>组成，其中一些层之后是<b>最大值池化层</b>，以及三个<b>全连接层</b>，最后是一个<b>softmax激活函数</b>有1000个输出的类别。为了使训练速度更快，我们使用了非饱和神经元<b>非饱和神经元</b>就是<b>ReLUs激活函数</b>和非常高效的GPU卷积运算实现。为了减少在全连接层中的过拟合，我们使用了一种最近开发的被证明非常有效的正则化方法--“dropout”。我们还在ILSVRC-2012比赛中加入了该模型的一个变体，并获得了15.3%的前5名测试错误率，而第二好的参赛者的错误率为26.2%。

## 1. 引言

首先，介绍了训练大量图片数据集的挑战性，自身实验需要用到大量图片数据，以及使用卷积神经网络的优势（CNN）：与标准的前馈神经网络相比，拥有更少的参数和连接层，并且在二维上更好的保留图片的特征信息，也会更好训练。

其次，作者介绍了本论文使用的网络以及提出的新计算特性。文中使用的方法用到了专门研究的为GPU计算使用的算法，提高了图像卷积计算的效率，并且降低了功耗和训练时间。由于网络的规模，作者采用了专门的方法解决过拟合问题。最终网络结构由5个卷积层和3个全连接层组成，并且作者提出：网络的规模和能力被GPU内存所限制，如果有更大内存的GPU和更多可用的数据集，可以在更复杂的网络上实现更加强大的功能。

代码

官方：[Google Code Archive - Long-term storage for Google Code Project Hosting.](https://code.google.com/archive/p/cuda-convnet/)

复现：[recurrent-paper/AletNet at master · YQ0Sun/recurrent-paper](https://github.com/YQ0Sun/recurrent-paper/tree/master/AletNet)

## 2. 研究背景

传统的物体识别方法依赖于机器学习技术，但受限于数据集的大小和多样性。为了提高识别性能，研究者们转向了更大规模的数据集，如 ImageNet，它包含了超过 1500 万张标记图像。

## 3. 数据集

论文描述了 ImageNet 数据集的构成，包括其图像来源、类别数量以及如何为 ILSVRC 准备数据集。

**ImageNet：**120万张训练图像，50000张验证图像和150000张测试图像，共约1000个类别的高分辨率图片。
**ILSVRC：**ILSVRC竞赛中唯一可以获得测试集标签的版本，文中模型报告了top-1和top-5（正解不在前五种最认可的标签里）两种rate。

对于ImageNet数据集，需要对分辨率不确定的图片进行预处理，文中统一缩放成了256*256的图片。另外对图像进行的唯一预处理就是在每个像素上减去训练集的均值。（像素去均值化是图像标准化的一种手段，可以移除掉图像中的平均亮度值，因为我们不关心图像本身的亮度。此外，研究表明去均值处理可以更加突出主要目标的轮廓等特征，如下图所示，天空纹理被去除了，汽车的轮廓则更明显。但是请注意，无论何种预处理，都只能用于训练集数据，绝不能用于测试集数据。）

![img](https://i-blog.csdnimg.cn/blog_migrate/1f93ecb723e96d83de8435ca9244c319.png)

## 4. 架构

### 4.1 ReLU非线性

这是作者认为网络中最重要的一环，使用的Relus激活函数。文中通过实验证明在同样的模型训练中，非饱和线性Relus的效率要高于饱和线性tanh函数（效率指达到相同准确率需要的epoch数量）。文中还强调更快的训练对于大的数据集和模型都是很大的积极作用的。

![1766410819152](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766410819152.png)

**网络体系架构：共计8个学习层：5个卷积层和3个全连接层**

![1766306918241](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766306918241.png)

• 该网络层包括8层权重，有五个卷积层和三个全连接层，最后通过 1000 路 softmax 输出分类概率。
• 引入了局部响应归一化（LRN）和重叠池化技术，增强了网络的泛化能力。

首先输入的是一张224 * 224 * 3（因为是彩色RGB三通道的图）

第一层用的卷积核的大小是 11∗11∗3 ，卷积核的个数是48+48=96，从这一层开始两个GPU开始分开运行，现在定义处理上半层特征图的叫GPU_A，处理下半层特征图的叫GPU_B，每个GPU负责48个卷积核的运算，上半层GPU_A生成48张特征图，下半层GPU_B生成48张特征图。这一层卷积结束之后，还需要LRN（Local Response Normalization 局部响应归一化）和Max_Pooling（最大池化）

第二层和第一层同理，两个GPU分别处理自己上一层传来的output（那48张特征图），卷积核的大小是 5∗5∗48 ，然后一共有128+128=256个卷积核，所以两个GPU各自利用自己上一层的output生成128张特征图。这一层的卷积结束之后还需要LRN（Local Response Normalization 局部响应归一化）和Max_Pooling（最大池化）

第三层和前两层不同，这一层两个GPU都要是将两个GPU的上一层的全部输出output作为输入input，所以这一层的卷积核大小是 3∗3∗ （128[来自GPU_A]+128[来自GPU_B]），也就是这层的卷积核是 3∗3∗256 ，而不是像前两层那样只是把自己上一层的输出当成输入，这层一共有192+192=384个卷积核，GPU_A负责前192个卷积核的生成的特征图，GPU_B负责后192个卷积核生成的特征图。

第四层和第五层同第二层。
关键来了，这里看图是接了一个全连接层(FC)，首先将128[来自GPU_A]和128[来自GPU_B]的一共256张特征图拉直成一个超长的向量，连接到一个大小为4096的全连接层中，其中4096个神经元的前2048个神经元由GPU_A运算，后2048个神经元由GPU_B来运算。

第七层和第六层同理。

第八层是再连接到一个大小为1000的全连接层中，用softmax，来算1000种分类的分布。

### 4.2多GPU部署

双GPU主要原因是当时受到硬件技术所限，所使用的GTX580只有3G内存，一块不足以支撑训练足够大的网络。但是好在显卡支持不同GPU之间直接访问内存，这就提供了并行训练的可能性。作者使用了两块GPU进行训练，每一块分别占有网络一半的kernel，且GPU之间只在特定的层进行通信，如特定层之间发送输入或输出数据才会进行通信。实践表明这样做不仅缩短了训练时间，而且小幅提升了最终的识别准确率（降低了错误率）


### **4.3 局部响应归一化**

Relu无需归一化输入，但是归一化输入有助于泛化，也有助于提升准确率。

![1766412672079](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766412672079.png)

其中， 𝑎(𝑥,𝑦)𝑗 ,是第j个通道上 x,y 位置上的ReLU激活值。n,k,α,β都是超参数，根据一组validation set得到n=5,k=2,α=10^−4,β=0.75，N是这个feature map总的通道个数。这里就是之前提到的“亮度去均值标准化”的过程。并且在实际数据集和额外测试数据集上，都证明这种局部响应规范化能带来更低的错误率。

加入这个机制的最初目的是，模仿真实神经元的侧抑制机制（lateral inhibition），提高网络的泛化能力。实际上，此机制能使top 1/5错误率下降了1.4/1.2%。经过此机制修正后，ReLU的正数部分也有类似于饱和非线性函数的曲线。ReLU+Local Response Normalization的输出，类似于真实神经元的电位发放的表现。

### **4.4 重叠 pooling**

**用在第一层卷积和第二层卷积**

在最大池化的基础上进行重叠池化， 这样和传统池化做法比信息压缩量小，信息丢失也就少，而且实验证明可以更好的避免过拟合。在文中也提到降低了错误率。

### 4.5 **总体结构**

介绍了网络中具体的层结构和层之间的规范化、数据传递的信息，具体可见结构图。

## 5.减少过拟合

### 5.1数据增强

大规模网络过拟合是正常现象，数据增强是常用的解决手段。

1、图像平移和水平翻转，从256 * 256图像随机裁出224 * 224图像，数据集扩展2048倍（但是扩展出来的图像都是高度依赖的）。

2、加全局调整，对整个训练集做PCA来取得RGB值的主要特征（改变RGB通道的强度），然后对输入进行主要特征的σ=0.1的高斯扰动。实际上相当于对图片的强度进行小范围调整。
**高斯扰动？**

每张图片的像素通道通过以下公式重新计算，并且每当再次用到这张图片时会重新计算。

![1766414281751](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766414281751.png)

pi和λi分别是rgb像素值的3×3协方差矩阵的第i个特征向量和特征值，并且αi是前述随机变量。捕捉了自然图像的一个重要特性，即物体的特性不因光照强度和颜色的变化而改变。

### **5.2Dropout**

![img](https://i-blog.csdnimg.cn/blog_migrate/690190aa8fbe3385de7768e4dc849db1.png)

a）标准神经网络 （b）运用后dropout后的神经网络

每个隐藏神经元输出时有一定的概率被赋值为0，在网络中就不在有贡献，也不会参与反向传播过程。本文中该概率选择的是0.5。神经元在随机被dropout的情况下学习的数据也是随机的，但是共享weights。因此网络的学习结果更具健壮性，也就解决了过分学习图像特征造成过拟合的现象。

**forward/前向过程：**可以看到神经元失活是先随机初始化一个与当前层规模一样的mask，然后根据mask中每个元素的随机值（介于0到1之间），将小于dropoutFraction的元素设置为0，其余为1，得到dropOutMask。再将该层的所有激活值与dropOutMask进行点乘，得到结果。

**backward/反向过程**：backprop回来的所在层的derivatives乘以对应层的dropOutMask。

**test过程**：所有神经元参与计算，但是每个元素值乘以(1-dropOutFraction)，作为没有被dropout的比率被留下的值。

## 6.学习细节

**• 权重初始化**：每层权重根据均值为0标准差为0.01的高斯分布初始化，第二、第四、第五层卷积和全连接层的bias初始化为常数1，剩下所有层的bias初始化为0。这种初始化有助于早期阶段正样本的学习的加速；
**• 优化算法**：小批量梯度下降；
**• 权重优化：**

![1766414818315](C:\Users\86159\AppData\Roaming\Typora\typora-user-images\1766414818315.png)

i是迭代次数，v是动量， 𝜖 是learning rate，后者是目标函数关于权重的偏导数，基于第i个batch的平均值。

• learning rate：0.01，所有层使用一样的初始学习率。训练过程中如果val-error不降低，那么就会对learning rate除以10。训练总共跑了90个epochs，3次因为val-error不降低调整学习率。
• epoch：90。

## 7.值得学习的点

1、在达到相同的准确率所需要的迭代次数，非饱和线性Relu的效率要高于饱和线性tanh函数。

2、卷积核使用的大小有什么讲究吗？第一层11、第二层5、第三层3（将两个GPU上的上一层输出作为输入）、第四层和第五层都是5。

3、提出在激活函数之前使用归一化思想可以有助于模型泛化，也可以提高准确率。这一点在现在的网络也应用广泛，比如常见的CBS，就会在激活函数前使用批量归一化。

4、重叠pooling：减少信息损失，避免过拟合(做实验得到的)。相邻池化窗口之间有重叠区域，此时一般sizeX > stride。

5、dropout，就是选择性地使神经元输出为0，从而使得神经元直接不能产生对于其他神经元的过度依赖，进而缓解过拟合。

5、网络的流程是如何的？网络形状是怎么变的？——> 输入是224 * 224 * 3，第一层卷积核为 11∗11∗3，卷积核的个数是48+48=96，第二层卷积核的大小是 5∗5∗48 ，一共有128+128=256个卷积核，第三层卷积核大小是 3∗3∗ （128[来自GPU_A]+128[来自GPU_B]=256），这层一共有192+192=384个卷积核，第四层卷积核为3∗3∗ 192，这层一共有192+192=384个卷积核，第五层卷积核为3∗3∗ 192，，这层一共有128+128=256个卷积核，第六层全连接层(FC)，首先将128[来自GPU_A]和128[来自GPU_B]的一共256张特征图拉直成一个超长的向量，连接到一个大小为4096的全连接层中，其中4096个神经元的前2048个神经元由GPU_A运算，后2048个神经元由GPU_B来运算。第七层同第六层，第八层是再连接到一个大小为1000的全连接层中，用softmax，来算1000种分类的分布。

6、激活函数ReLu、sigmoid、tanh的区别
对应的公式分别为：

ReLu：y = max(0, x) ∀y∈(0,无穷)
sigmoid：y = 1/(1 + e-x) ∀y∈(0,1)
tanh： y = (ex - e-x)/(ex + e-x) b ∀y∈(−1,1)
使用场景：
1、在输出层，一般会使用sigmoid函数，因为一般期望的输出结果概率在0~1之间，如二分类，sigmoid可作为输出层的激活函数
2、在隐藏层，tanh函数优于sigmoid函数。因为其取值范围介于-1 ~ 1之间，有类似数据中心化的效果。
3、但实际应用中，tanh和sigmoid会在端值趋于饱和，造成训练速度减慢，故一般的深层网络的激活函数默认大多采用relu函数，也可以前面几层使用relu函数，后面几层使用sigmoid函数。

7、局部响应归一化和批量归一化都使用了数据映射的方法，这两者有什么区别？